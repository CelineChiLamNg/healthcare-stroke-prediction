{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h1><center>Stroke Prediction</center></h1>\n",
    "<center>September 2024</center>\n",
    "<center>Celine Ng</center>"
   ],
   "id": "7046d3df85bd27e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Project Introduction   \n",
    "    1. Notebook Preparation\n",
    "    1. Motivation and Objectives\n",
    "    1. Dataset\n",
    "1. Data Cleaning\n",
    "    1. Duplicate rows\n",
    "    1. Datatypes\n",
    "    1. Unique values\n",
    "    1. Missing values\n",
    "1. EDA\n",
    "    1. Distribution\n",
    "    1. Distribution according to target label\n",
    "    1. Missing Values\n",
    "    1. Statistic Inference\n",
    "        1. Target Population\n",
    "        2. Transform 'AnnualIncome'\n",
    "       3. Hypothesis Testing\n",
    "1. Data Formatting\n",
    "1. Preprocessing\n",
    "    1. Transformations\n",
    "    1. Data Splitting\n",
    "1. Models\n",
    "    1. Basic model\n",
    "    1. Baseline model\n",
    "    1. Hyperparameter Tuning\n",
    "1. Conclusion\n",
    "1. Improvements"
   ],
   "id": "a5e367a9937a2716"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Project Introduction",
   "id": "324725f2163fc7f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1. Notebook Preparation",
   "id": "7a982029f037f304"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "%pip install -r requirements.txt"
   ],
   "id": "3551992471e41976",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils.eda import *\n",
    "from utils.model import *\n",
    "\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.model_selection import (train_test_split, StratifiedKFold, \n",
    "                                     cross_validate, GridSearchCV)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (OneHotEncoder, FunctionTransformer,\n",
    "                                   StandardScaler)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, f1_score, \n",
    "                             precision_score, recall_score, make_scorer)"
   ],
   "id": "fe67298feee71bfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2. Motivations and Objectives",
   "id": "7e4d6c6e5b024711"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This project's objectives are: \n",
    "<br><br>\n",
    "1. Practice performing EDA.\n",
    "2. Practice applying statistical inference procedures.\n",
    "3. Practice using various types of machine learning models.\n",
    "4. Practice building ensembles of machine learning models.\n",
    "5. Practice deploying machine learning models."
   ],
   "id": "91bbe7e4a627ff64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.3. Dataset\n",
    "\n",
    "Objective: Brief overview of our dataset, including the features and label"
   ],
   "id": "c4f28634cbf86ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset was downloaded from Kaggle, [Stroke_Prediction_Data](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset)\n",
    ",<br>\n",
    "on 10 September 2024. It will be used to predict whether a patient is likely\n",
    " <br> to get a stroke. <br>\n",
    "The data contains 11 clinical features, like gender, age, smoking status, <br>\n",
    "etc, that help describe each patient."
   ],
   "id": "688e85ca07589fce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Features: <br>\n",
    "1) id: unique identifier\n",
    "2) gender: \"Male\", \"Female\" or \"Other\"\n",
    "3) age: age of the patient\n",
    "4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
    "5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
    "6) ever_married: \"No\" or \"Yes\"\n",
    "7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
    "8) Residence_type: \"Rural\" or \"Urban\"\n",
    "9) avg_glucose_level: average glucose level in blood\n",
    "10) bmi: body mass index\n",
    "11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
    "<br>\n",
    "\n",
    "Label: <br>\n",
    "stroke: 1 if the patient had a stroke or 0 if not <br>"
   ],
   "id": "92f8134630a86d1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stroke_data = pd.read_csv('data/healthcare-dataset-stroke-data.csv')\n",
    "display(stroke_data.head())\n",
    "shape = stroke_data.shape\n",
    "print(f'Number of rows: {shape[0]}\\nNumber of columns: {shape[1]}')"
   ],
   "id": "68d8ec22db22108d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.  Data cleaning\n",
    "Objective:\n",
    "1. Closer look at the values that consist of our data\n",
    "2. Look out for duplicates, and missing and/or unusual values"
   ],
   "id": "c2a3a4ba343554eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1. Duplicate rows",
   "id": "d078915f787f206b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(stroke_data.id.duplicated().any())",
   "id": "a2f4b16548f4345f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After confirming there is no duplicated id number/cases, since id number \n",
    "should not be relevant information to base our prediction on, we can remove it."
   ],
   "id": "2f6bd5c1acf48c17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "stroke_data = stroke_data.drop(columns=['id'])",
   "id": "37c9762b889f90cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2. Datatypes",
   "id": "c3570d94711b3a6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datatype_data = stroke_data.dtypes\n",
    "datatype_data"
   ],
   "id": "fc2d5829dfb69550",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Certain features have unexpected datatypes, like age, hypertension, and \n",
    "heart_disease. Looking into the values itself will help clarify data types."
   ],
   "id": "a0a42e0e73179d03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.3. Unique values",
   "id": "21fe42a29dc5e2bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "stroke_data.nunique()",
   "id": "88dc0d4661b96d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some features have unexpected amount of unique values. Looking into the \n",
    "values itself will help.<br>"
   ],
   "id": "c34fe97fa424246f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for column in stroke_data.columns:\n",
    "    df = stroke_data[column].value_counts()\n",
    "    display(df)"
   ],
   "id": "c45bc642a194119d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Looking more into work_type registered as 'children', as the difference \n",
    "between 'children' and 'Never_worked' is not clear."
   ],
   "id": "31619b563702bef4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "stroke_data[stroke_data.work_type=='children'].age.describe()",
   "id": "467cc8380a658ed0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "All labeled as children are <= 16 years old.",
   "id": "e1618f0e7c1f3af0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "stroke_data[(stroke_data.age<=16)]['work_type'].value_counts()",
   "id": "f25196b8b12ce8d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It is clear that not all under 16 year olds are labeled as 'children'. Some \n",
    "are 'Never_worked', and some are working. It is unclear why only part is \n",
    "labeled as 'children'. <br>However, it since this\n",
    " feature is to state their work status and 'children' in this context would \n",
    " mean not working, we can consolidate 'children' into 'Never_worked'. <br>\n",
    " This would ensure consistency across all age groups, without losing the age\n",
    "  information, as it is in the 'age' column."
   ],
   "id": "3c0b4710f29e35ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stroke_data.loc[stroke_data['work_type'] == 'children', 'work_type'] = \\\n",
    "    'Never_worked'"
   ],
   "id": "1dfc797a0a5811ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.4. Missing values",
   "id": "c8e5cc192f7617cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "missing_values(df=stroke_data)",
   "id": "c03098f591c0e9c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To properly impute missing values, it is necessary to first understand the \n",
    "sampling population and if the missing values belong to a specific subset of\n",
    " sampling population."
   ],
   "id": "20854b7f97ba8040"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HTML('''\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Data Cleaning Insights:</b><br>\n",
    "    1. There are no duplicated rows, and each row is identified by a unique ID.<br>\n",
    "    2. The feature 'id' is irrelevant to our stroke prediction, so it was removed.<br>\n",
    "    3. Consolidated 'children' work type to 'Never_worked'. <br>\n",
    "    4. All missing values come from the 'bmi' feature, and further analysis \n",
    "    is needed to determine proper imputation methods.\n",
    "</div>\n",
    "''')"
   ],
   "id": "b649657d3e6e5e5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. EDA\n",
    "\n",
    "Objectives:\n",
    "1. Data distribution\n",
    "2. Check and handle outliers, missing values\n",
    "3. Comparison between data with and without stroke\n",
    "4. Understand how our data was collected and possible bias\n",
    "5. Understand how do columns related with each other - correlation\n",
    "6. "
   ],
   "id": "2c4b1c3744e03d5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1. Distribution",
   "id": "6cfd7aee07e08876"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Identify features, label, and different feature types.",
   "id": "b2efdfaf00a45456"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_name = 'stroke'\n",
    "data, target = (stroke_data.drop(columns=[target_name]), \n",
    "                stroke_data[target_name])"
   ],
   "id": "9019c01f681607c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorical_columns_selector = selector(dtype_exclude='float64')\n",
    "numerical_columns_selector = selector(dtype_include='float64')\n",
    "\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "numerical_columns = numerical_columns_selector(data)"
   ],
   "id": "fc6c652a3d3001ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Numerical Features**",
   "id": "b0398eae6addb48d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data[numerical_columns].describe()",
   "id": "81481e8334c7d329",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Plotting boxplot over violin plot for numerical features to have a visual \n",
    "understanding of the overall distribution, range, and outliers."
   ],
   "id": "9ee1d7a1edbdcb10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "violin_boxplot(data=data, columns=numerical_columns, title='Numerical Data '\n",
    "                                                           'Violin Box '\n",
    "                                                           'Plot')\n",
    "plt.show()"
   ],
   "id": "336d0b1e6684d83a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Categorical Features**",
   "id": "1147ab7a911c1848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "categorical_columns",
   "id": "81545b879a31d0ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Represent categorical features in barplots, visualize unique values for each\n",
    " feature and their distribution in percentage."
   ],
   "id": "7417135ff614cf62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "percentage_subplots(data=data, columns=categorical_columns, title='Categorical '\n",
    "                                                             'data '\n",
    "                                                             'Distribution',\n",
    "               nrows=3, ncols=3)"
   ],
   "id": "30f61df72ed27626",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Target Variable**",
   "id": "4e35f9df19d0e1cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Target variable and its distribution in percentage.",
   "id": "6bb9d7c0d2585846"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "percentage_subplots(data=stroke_data, columns=['stroke'], title='Target Variable')",
   "id": "bfb73cdba515cd35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HTML('''\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Distribution Insights:</b><br>\n",
    "    <i>numerical features...</i><br>\n",
    "    1. Numerical features show very different ranges of values, signalling an \n",
    "    importance in scaling in future feature engineering.<br>\n",
    "    2. Average Glucose Level has the most outliers, which can be justified \n",
    "    by the bimodal distribution. This could contain relevant information to \n",
    "    our target prediction.\n",
    "    <br>\n",
    "    3. 'age' includes children less than 1 year old, justifies why the \n",
    "    datatype is float.<br><br>\n",
    "    \n",
    "    <i>for categorical features...</i><br>\n",
    "    4. Inconsistent labeling among categorical features, preprocessing \n",
    "    required before applying learning model.<br>\n",
    "    5. Outlier responses in 'gender'.<br><br>\n",
    "    \n",
    "    <i>and the target variable...</i><br>\n",
    "    6. Imbalanced dataset, with only 4.87% responding positively to stroke. \n",
    "    <br>\n",
    "</div>\n",
    "''')"
   ],
   "id": "5e0e277398878f7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HTML('''\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Target Population:</b><br>\n",
    "    The target population is mostly likely the group of people who are \n",
    "    concerned about their probability of getting a stroke.<br><br>\n",
    "    \n",
    "    <b>Sampling Population:</b><br>\n",
    "    Our data's distribution describes our sampling population. \n",
    "    <br><br>\n",
    "    \n",
    "    <b>Potential Bias:</b><br>\n",
    "    The data's collection method and answer's definition are unknown. \n",
    "    If the sampling population is not representative of the target \n",
    "    population, there might be significant biases. \n",
    "    <br>\n",
    "</div>\n",
    "''')"
   ],
   "id": "514a48cf9d199136",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.2. Distribution according to target label\n",
    "Objective:<br>\n",
    "Plot data distribution by target variable to understand if the populations \n",
    "show clear differences in their characteristics."
   ],
   "id": "b7a5c9e4aa93891d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Numerical Features** <br>\n",
    "Cases with stroke"
   ],
   "id": "9b0aee12efdf9b8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "stroke_data[stroke_data.stroke == 1][numerical_columns].describe()",
   "id": "547c93380fecb808",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cases without stroke",
   "id": "2d69df21d627f978"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "stroke_data[stroke_data.stroke == 0][numerical_columns].describe()",
   "id": "35e358bb92289fac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), sharex=True)\n",
    "\n",
    "violin_boxplot(data=stroke_data[stroke_data.stroke == 1], \n",
    "               columns=numerical_columns, title='stroke - True', ax=axes[1])\n",
    "violin_boxplot(data=stroke_data[stroke_data.stroke == 0], \n",
    "               columns=numerical_columns, title='stroke - False', ax=axes[0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ef0624c2a9c736f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "stacked_horizontal_feature_distribution(data=stroke_data[stroke_data.stroke == 1], \n",
    "                                        columns=categorical_columns, \n",
    "                                        title='Categorical Features '\n",
    "                                              'Distribution\\nstroke'\n",
    "                                              ' - True') \n",
    "stacked_horizontal_feature_distribution(data=stroke_data[stroke_data.stroke == 0], \n",
    "               columns=categorical_columns, title='Categorical Features '\n",
    "                                              'Distribution\\nstroke - False')"
   ],
   "id": "f7e1c582f5588a86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HTML('''\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Distribution Insights:</b><br>\n",
    "    Plots show visually significant difference between population with and \n",
    "    without stroke.<br><br>\n",
    "    <i>numerical features...</i><br>\n",
    "    1. With stroke consists of a higher age and higher bmi mean, with a \n",
    "    smaller standard deviation. Considering the sample size is smaller for \n",
    "    cases with stroke, it is very possible that these 2 features are highly \n",
    "    predictive of our target variable.<br>\n",
    "    2. Average Glucose Level for cases with stroke has both the higher mean \n",
    "    and higher standard deviation. This is likely due to its clear \n",
    "    bimodal distribution. <br><br>\n",
    "\n",
    "    \n",
    "    <i>for categorical features...</i><br>\n",
    "    3.  Smoking status, residence type, and gender seem similar among both \n",
    "    target labels. This could mean these features do not contain predictive \n",
    "    information for the target variable.<br>\n",
    "    4. Work type, ever married, heart disease, and hypertension show \n",
    "    more different distribution.<br>\n",
    "    5. Knowing that age seem to be an important factor, important to note \n",
    "    that some other features are also highly influenced by age. For example,\n",
    "     it is more likely for the older age group to have a higher ratio to have \n",
    "    ever been married, or worked. <br>\n",
    "</div>\n",
    "''')"
   ],
   "id": "d101a2e0f058306e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.3. Missing values\n",
    "\n",
    "Objective: <br>\n",
    "All missing values come from the 'bmi' feature. Determine proper treatment."
   ],
   "id": "4dc5192610dd8596"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Plot data distribution of cases with missing bmi values. Compare to the \n",
    "original data distributions, and check if there is a clear difference \n",
    "with missing bmi cases."
   ],
   "id": "f3e447f2c74318cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "violin_boxplot(data=stroke_data[stroke_data.bmi.isnull()], \n",
    "               columns=['age', 'avg_glucose_level'],\n",
    "               title='Numerical Data Violin Box Plot\\nbmi missing - True')"
   ],
   "id": "731a314d3610ece",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stacked_horizontal_feature_distribution(data=stroke_data[stroke_data.bmi.isnull()], \n",
    "                                        columns=categorical_columns, \n",
    "                                        title='Numerical Data Violin Box'\n",
    "                                              'Plot\\nbmi missing - True')"
   ],
   "id": "1523990927695374",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The distribution for both numerical and categorical do not seem to represent\n",
    " a different subpopulation. <br>\n",
    "Check if bmi is correlated with other features, this can help choose what \n",
    "values to impute."
   ],
   "id": "ac26f2f8ecbecc7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.lmplot(data=stroke_data, x='avg_glucose_level', y='bmi', hue='stroke')\n",
    "sns.lmplot(data=stroke_data, x='age', y='bmi', hue='stroke')"
   ],
   "id": "2166a97b7c4dafd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "correlation_matrix = stroke_data[numerical_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, \n",
    "            center=0)\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "plt.show()"
   ],
   "id": "868d488b22b358f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the plots above: <br>\n",
    "1. Correlation between numerical features are very weak to weak.\n",
    "2. bmi values could be imputed by linear interpolation from other numerical \n",
    "features. Since correlation coefficient is low, using mean imputation or \n",
    "leaving it as null values is also good enough."
   ],
   "id": "3b05dd4570f949e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HTML('''\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Missing Values Insights:</b><br>\n",
    "    After some analysis, it is understood that missing values do not belong \n",
    "    to any specific subpopulation. The subset of data which has missing \n",
    "    'bmi' values do not show a drastically different distribution in other \n",
    "    features. Therefore, imputing with the mean value, or even keeping it as\n",
    "    null value during the preprocessing step will be enough.\n",
    "</div>\n",
    "''')"
   ],
   "id": "9a3da32baabe190e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Data Formatting\n",
    "Objective: Only decision tree based models will be used in this project, \n",
    "so the following formatting is only to be able to draw a tree visually, \n",
    "providing with better interpretability in the end."
   ],
   "id": "ba1d85a23eaffe2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove all empty values in values.",
   "id": "25a1d332cb13af89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stroke_data['smoking_status'] =(\n",
    "    stroke_data['smoking_status'].replace(' ', '_', regex=True))\n",
    "stroke_data['smoking_status'].unique()"
   ],
   "id": "65214ccfd8c9c53b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set empty values to 0 ",
   "id": "c3ecb82a08319157"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "stroke_data['bmi'] = stroke_data['bmi'].fillna(0)",
   "id": "ae566343edef4db0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Preprocessing\n",
    "Objective:<br>\n",
    "Prepare preprocesses in order to apply learning models. <br>\n",
    "1. Data splitting\n",
    "2. Data transformation and encoding"
   ],
   "id": "79e664d42fcf1b1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.1. Data Splitting",
   "id": "3aefa0b430b05690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X, y = (stroke_data.drop(columns=[target_name]), \n",
    "                stroke_data[target_name])"
   ],
   "id": "36a55955df21a6e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Split data into train, validation, and test sets. With a ratio of 20% being \n",
    "the test set."
   ],
   "id": "794701cbef4775fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_val, X_test, y_train_val, y_test =\\\n",
    "    train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0)"
   ],
   "id": "54ec6ecf67e514a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To evaluate variability of our estimation of the performance, cross \n",
    "validation will be used. More specifically, Stratified KFold, for the train \n",
    "and validation sets, as this is a better option for imbalanced and \n",
    "small datasets. "
   ],
   "id": "807f77392ec461f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)",
   "id": "a378c5b7e09148a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.2. Data Transformation",
   "id": "1c29e59036efa170"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For models based on decision trees, transformations like scaling and \n",
    "normalizing will not be needed, only encoding categorical columns will be \n",
    "necessary. But we would want to compare results with a basic logistic \n",
    "regression, so we will apply StandardScaler()."
   ],
   "id": "f120029b44a3912f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Define columns**",
   "id": "db5ba9f840b90417"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "multicat_columns = ['gender', 'work_type', 'smoking_status']\n",
    "binary_columns = ['hypertension', 'heart_disease', 'ever_married', \n",
    "                 'Residence_type']"
   ],
   "id": "2f1cd8c212d1a05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Define preprocessors for each type of data**<br>\n",
    "LabelEncoder does not work within a pipeline, write our own function."
   ],
   "id": "c8afa110193c9276"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "multicat_preprocessor = OneHotEncoder(handle_unknown='ignore')\n",
    "binary_preprocessor = FunctionTransformer(binary_label_encoding)\n",
    "numeric_preprocessor = StandardScaler()"
   ],
   "id": "4cc36ad7fa66eca5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Bundle preprocessing for all data**",
   "id": "986bb126f31327fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', multicat_preprocessor, multicat_columns),\n",
    "        ('binary', binary_preprocessor, binary_columns),\n",
    "        ('numeric', numeric_preprocessor, numerical_columns)\n",
    "    ]\n",
    ")"
   ],
   "id": "763214800c708296",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Models\n",
    "Objective: <br>\n",
    "1. Apply and hyperparameter tuning machine learning models\n",
    "2. Assess and select the best model"
   ],
   "id": "7a898e33e260f9fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Models**\n",
    "1. Random forest classifier\n",
    "2. XGBoost gradient boosted trees\n",
    "3. LightGBM histogram gradient boosting\n",
    "4. Logistic regression"
   ],
   "id": "e7c2b457e64f29a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random_forest = RandomForestClassifier(random_state=0)\n",
    "clf_xbg = xgb.XGBClassifier(objective='binary:logistic', missing=0, \n",
    "                            random_state=0)\n",
    "clf_lgb = lgb.LGBMClassifier(objective='binary', random_state=0)\n",
    "log_reg = LogisticRegression(random_state=0)"
   ],
   "id": "17451576bdc1b67a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "stroke_data.isnull().any().any()",
   "id": "f3bd102cbccff4bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Pipeline**",
   "id": "f842acc3e0945207"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', clf_xbg)\n",
    "])\n",
    "\n",
    "pipeline_lgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', clf_lgb)\n",
    "])\n",
    "\n",
    "pipeline_lr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', log_reg)\n",
    "])"
   ],
   "id": "7a3824e9a5f4da62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.1. Basic model\n",
    "Objective: Fit and evaluate the basic model with default parameters to gain \n",
    "intuition on what improvements need to be made."
   ],
   "id": "3a3ef9a3240be442"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cv_results_rf = cross_validate(\n",
    "    pipeline_rf, X_train_val, y_train_val, cv=skf,\n",
    "    scoring=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n",
    "    return_train_score=True, n_jobs=2\n",
    ")\n",
    "\n",
    "cv_results_xgb = cross_validate(\n",
    "    pipeline_xgb, X_train_val, y_train_val, cv=skf,\n",
    "    scoring=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'], \n",
    "    return_train_score=True, n_jobs=2\n",
    ")\n",
    "\n",
    "cv_results_lgb = cross_validate(\n",
    "    pipeline_lgb, X_train_val, y_train_val, cv=skf,\n",
    "    scoring=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'], \n",
    "    return_train_score=True, n_jobs=2\n",
    ")\n",
    "\n",
    "cv_results_lr = cross_validate(\n",
    "    pipeline_lr, X_train_val, y_train_val, cv=skf,\n",
    "    scoring=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'], \n",
    "    return_train_score=True, n_jobs=2\n",
    ")\n",
    "\n",
    "print(f\"cv_results_rf{cv_results_rf}\")\n",
    "print(f\"cv_results_xgb{cv_results_xgb}\")\n",
    "print(f\"cv_results_lgb{cv_results_lgb}\")\n",
    "print(f\"cv_results_lr{cv_results_lr}\")"
   ],
   "id": "f8548ced273ad9ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_rf = pd.DataFrame(cv_results_rf).mean().to_frame('Random Forest')\n",
    "df_xgb = pd.DataFrame(cv_results_xgb).mean().to_frame('XGBoost')\n",
    "df_lgb = pd.DataFrame(cv_results_lgb).mean().to_frame('LightGBM')\n",
    "df_lr = pd.DataFrame(cv_results_lr).mean().to_frame('Logistic Regression')\n",
    "\n",
    "results_df = pd.concat([df_rf, df_xgb, df_lgb, df_lr], axis=1)\n",
    "results_df"
   ],
   "id": "f2b37a3d74261a70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HTML('''\n",
    "<div class=\"alert alert-block alert-info\">\n",
    " <b>Fit and Score Time:</b><br>\n",
    "    XGBoost is also the fastest in terms of score time, followed by \n",
    "LightGBM, and Random Forest is slightly slower. XGBoost and LightGBM are \n",
    "designed with optimizations that make predictions very fast, which explains \n",
    "why their scoring times are lower than Random Forest. LightGBM is also \n",
    "efficient but may benefit from more tuning, as its fit and score time are \n",
    "unexpectedly higher than XGBoost. <br><br>\n",
    "<b>Test Scores:</b><br>\n",
    "<i>For a problem like ours where main goal is to predict \n",
    "whether a person is likely or not to have stroke, it is preferable to have \n",
    "more FALSE POSITIVES than FALSE NEGATIVES.</i><br>\n",
    "1. Accuracy: In our dataset due to how imbalanced it is, accuracy is not a good\n",
    "indicator to rely on. <br>\n",
    "2. Precision: This metric represents the accuracy of the positive \n",
    "predictions. This value is consistently low on all models, being 0.23 from \n",
    "XGBoost the highest. Considering the nature of the problem, this is not a \n",
    "huge issue. <br>\n",
    "3. Recall: This is one of the most important metric in our case, as it \n",
    "captures how often the model correctly identifies a case with stroke from all \n",
    "cases with stroke. Currently this value is extremely low on all models.<br>\n",
    "4. Area under the curve (AUC ROC): This metric measures how well the model \n",
    "distinguishes between positive and negative cases. LightGBM has the best \n",
    "results of all 3.<br>\n",
    "5. F1 Score: This metric balances how well both recall and precision scored.\n",
    " Even though precision is not the focus of our problem, it is still ideal to\n",
    "  have a model which performs better on both. And it is currently scoring \n",
    "  low on all models.\n",
    "</div>\n",
    "''')"
   ],
   "id": "6c493c63c0df3a3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.2. Baseline model\n",
    "Objective: Our dataset is very imbalanced, more than 95% of data belongs to \n",
    "no stroke (0). If the model simply predicted every case with no stroke, it \n",
    "would still get a very high score. Therefor, we will compare obtained scores\n",
    " to this baseline, to understand how much better actually are the models."
   ],
   "id": "5ee14c2d28e45fc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "baseline_model = DummyClassifier(strategy='constant', constant=0)\n",
    "\n",
    "baseline_results = cross_validate(\n",
    "    baseline_model, X_train_val, y_train_val, cv=skf, \n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': make_scorer(precision_score, zero_division=0),\n",
    "        'recall': 'recall',\n",
    "        'f1': make_scorer(f1_score, zero_division=0)\n",
    "    }, \n",
    "    return_train_score=True, n_jobs=2\n",
    ")\n",
    "baseline_df = pd.DataFrame(baseline_results).mean().to_frame('Baseline')\n",
    "baseline_df"
   ],
   "id": "92bbcf73d9308777",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HTML('''\n",
    "<div class=\"alert alert-block alert-info\">\n",
    " <b>Baseline Model:</b><br>\n",
    " As expected, if the model predicted all values to be 0, no stroke, the \n",
    " accuracy score would be very high, 0.95 in this case is actually higher \n",
    " than XGBoost and LightGBM. <br>As the \n",
    " DummyClassifier does not distinguish between positive and negative cases, \n",
    " it is also predictable that AUC is 0.5, as good as random guesses would do.\n",
    " </div>\n",
    "''')"
   ],
   "id": "2c65e2bd58f3886c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.3. Hyperparameter tuning",
   "id": "9229ba629a6fd3cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**First tuning**",
   "id": "277f519b0319c5ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The first hyperparameter tuning resulted in the following **best \n",
    "parameters** and **best recall scores**: <br><br>\n",
    "*It is clear that the models are not good at identifying the actual \n",
    "positive cases, which is the goal of the project.*\n",
    "\n",
    "1. param_grid_rf = { <br>\n",
    "    'classifier__n_estimators': [100, 200], <br>\n",
    "    'classifier__max_depth': [5, 10, None], <br>\n",
    "    'classifier__min_samples_split': [2, 10],<br>\n",
    "    'classifier__min_samples_leaf': [1, 5] <br>\n",
    "}<br>\n",
    "Random Forest Best Parameters: {'classifier__max_depth': 10, \n",
    "'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, \n",
    "'classifier__n_estimators': 100} <br><br>\n",
    "\n",
    "2. param_grid_xgb = {<br>\n",
    "    'classifier__n_estimators': [100, 200], <br>\n",
    "    'classifier__max_depth': [3, 6, 10], <br>\n",
    "    'classifier__learning_rate': [0.01, 0.1], <br>\n",
    "    'classifier__subsample': [0.8, 1.0]<br>\n",
    "}<br>\n",
    "XGBoost Best estimator: {'classifier__learning_rate': 0.1, \n",
    "'classifier__max_depth': 6, 'classifier__n_estimators': 100, \n",
    "'classifier__subsample': 0.8}<br><br>\n",
    "\n",
    "3. param_grid_lgb = {<br>\n",
    "    'classifier__n_estimators': [100, 200],<br>\n",
    "    'classifier__max_depth': [5, 10],<br>\n",
    "    'classifier__learning_rate': [0.01, 0.1],<br>\n",
    "    'classifier__num_leaves': [31, 50]<br>\n",
    "}<br>\n",
    "LightGBM Best estimator: {'classifier__learning_rate': 0.1, \n",
    "'classifier__max_depth': 5, 'classifier__n_estimators': 200, \n",
    "'classifier__num_leaves': 31}<br>\n",
    "\n",
    "\n",
    "Random Forest Best recall score: 0.020512820512820513<br>\n",
    "XGBoost Best recall score: 0.0923076923076923<br>\n",
    "LightGBM Best recall score: 0.08205128205128204<br>"
   ],
   "id": "289b97c87af4780d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Second tuning**<br>\n",
    "\n",
    "1. Random Forest Best estimator: {'classifier__max_depth': 12, \n",
    "'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, \n",
    "'classifier__n_estimators': 50} <br>\n",
    "2. XGBoost Best estimator: {'classifier__learning_rate': 0.12, \n",
    "'classifier__max_depth': 4, 'classifier__n_estimators': 120, \n",
    "'classifier__subsample': 0.8} <br>\n",
    "3. LightGBM Best estimator: {'classifier__learning_rate': 0.15, \n",
    "'classifier__max_depth': 5, 'classifier__n_estimators': 250, \n",
    "'classifier__num_leaves': 20} <br>\n",
    "\n",
    "Random Forest Best recall score: 0.03076923076923077 <br>\n",
    "XGBoost Best recall score: 0.10256410256410256<br>\n",
    "LightGBM Best recall score: 0.10256410256410256<br>"
   ],
   "id": "27670fe8fc79b2e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Third tuning**<br>\n",
    "\n",
    "1. Random Forest Best estimator: {'classifier__max_depth': 15, \n",
    "'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 3, \n",
    "'classifier__n_estimators': 30} <br>\n",
    "2. XGBoost Best estimator: {'classifier__learning_rate': 0.12, \n",
    "'classifier__max_depth': 4, 'classifier__n_estimators': 150, \n",
    "'classifier__subsample': 0.7} <br>\n",
    "3. LightGBM Best estimator: {'classifier__learning_rate': 0.2, \n",
    "'classifier__max_depth': 5, 'classifier__n_estimators': 220, \n",
    "'classifier__num_leaves': 15} <br>\n",
    "\n",
    "Random Forest Best recall score: 0.035897435897435895<br>\n",
    "XGBoost Best recall score: 0.10769230769230768<br>\n",
    "LightGBM Best recall score: 0.11794871794871793<br>"
   ],
   "id": "f310d1eaf6fef58b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Forth tuning**",
   "id": "a0dae8505849c827"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_grid_rf = { \n",
    "    'classifier__n_estimators': [20, 30, 50],\n",
    "    'classifier__max_depth': [12, 15],\n",
    "    'classifier__min_samples_split': [2, 3],\n",
    "    'classifier__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [150, 200],\n",
    "    'classifier__max_depth': [2, 4, 5],\n",
    "    'classifier__learning_rate': [0.11, 0.12, 0.13],\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8]\n",
    "}\n",
    "\n",
    "param_grid_lgb = {\n",
    "    'classifier__n_estimators': [200, 220, 250],\n",
    "    'classifier__max_depth': [4, 5, 6],\n",
    "    'classifier__learning_rate': [0.15, 0.2, 0.3],\n",
    "    'classifier__num_leaves': [5, 10, 15]\n",
    "}"
   ],
   "id": "d8fd62d1d1bc36b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scoring_metrics = {\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': 'recall',\n",
    "    'f1': make_scorer(f1_score, zero_division=0)\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=skf, \n",
    "                       scoring=scoring_metrics, refit='recall', verbose=10)\n",
    "grid_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, cv=skf, \n",
    "                        scoring=scoring_metrics, refit='recall', verbose=10)\n",
    "grid_lgb = GridSearchCV(pipeline_lgb, param_grid_lgb, cv=skf, \n",
    "                        scoring=scoring_metrics, refit='recall', verbose=10)\n",
    "\n",
    "grid_rf.fit(X_train_val, y_train_val)\n",
    "grid_xgb.fit(X_train_val, y_train_val)\n",
    "grid_lgb.fit(X_train_val, y_train_val)\n",
    "\n",
    "print(\"Random Forest Best estimator:\", grid_rf.best_params_)\n",
    "print(\"XGBoost Best estimator:\", grid_xgb.best_params_)\n",
    "print(\"LightGBM Best estimator:\", grid_lgb.best_params_)"
   ],
   "id": "bbb6e86e5215bb4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Print out recall score for the best parameters found after 4 iterations of \n",
    "Hyperparameter Tuning."
   ],
   "id": "d75ed937a8a58346"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-03T16:27:13.884831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Random Forest Best recall score:\", grid_rf.best_score_)\n",
    "print(\"XGBoost Best recall score:\", grid_xgb.best_score_)\n",
    "print(\"LightGBM Best recall score:\", grid_lgb.best_score_)"
   ],
   "id": "37b5fbc507462083",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the newly fitted model's parameters",
   "id": "6a8624f2e79e734"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-03T16:27:13.885290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_rf_2 = grid_rf.best_estimator_\n",
    "best_xgb_2 = grid_xgb.best_estimator_\n",
    "best_lgb_2 = grid_lgb.best_estimator_"
   ],
   "id": "e9f3dca8b4249f47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-03T16:27:13.886174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HTML('''\n",
    "<div class=\"alert alert-block alert-info\">\n",
    " <b>Hyperparameter Tuning:</b><br>\n",
    " After 4 iterations of tuning hyperparameters, it has resulted in better \n",
    " performance of the model with the training and validation set.<br><br>\n",
    " The best parameters the models have at this stage: <br>\n",
    " 1. Random Forest Best estimator: {'classifier__max_depth': 15, \n",
    " 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 3, \n",
    " 'classifier__n_estimators': 30}<br>\n",
    "2. XGBoost Best estimator: {'classifier__learning_rate': 0.12, \n",
    "'classifier__max_depth': 4, 'classifier__n_estimators': 200, \n",
    "'classifier__subsample': 0.8}<br>\n",
    "3. LightGBM Best estimator: {'classifier__learning_rate': 0.3, \n",
    "'classifier__max_depth': 6, 'classifier__n_estimators': 220, \n",
    "'classifier__num_leaves': 10}<br> <br>\n",
    "\n",
    "With the following recall score on validation set:<br>\n",
    "1. Random Forest Best recall score: 0.035897435897435895<br>\n",
    "2. XGBoost Best recall score: 0.12307692307692308<br>\n",
    "3. LightGBM Best recall score: 0.1282051282051282<br><br>\n",
    "\n",
    "In general:<br>\n",
    "XGBoost and LightGBM have very similar recall scores with the validation \n",
    "set, 12.3% and 12.8% respectively. Random Forest had the worst scoring, \n",
    "almost with only 3.60%. Considering that recall is an important metric in \n",
    "our case, RandomForest would not be considered for further comparison.<br><br>\n",
    " </div>\n",
    "''')"
   ],
   "id": "f331a5a3ac267eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.4. Select the best model\n",
   "id": "ca664086deb33bda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-03T16:27:13.887012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_xgb = best_xgb_2.predict(X_test)\n",
    "y_pred_lgb = best_lgb_2.predict(X_test)"
   ],
   "id": "bccbc55c14ba4317",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T16:27:13.936820Z",
     "start_time": "2024-10-03T16:27:13.888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Random Forest - Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest - ROC AUC:\", roc_auc_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest - Precision:\", precision_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest - Recall:\", recall_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest - F1 Score:\", f1_score(y_test, y_pred_rf))"
   ],
   "id": "9426de9011f9491a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-03T16:27:13.889159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "print(\"Random Forest Performance:\")\n",
    "evaluate_model(grid_rf, X_test, y_test)\n",
    "\n",
    "print(\"\\nXGBoost Performance:\")\n",
    "evaluate_model(grid_xgb, X_test, y_test)\n",
    "\n",
    "print(\"\\nLightGBM Performance:\")\n",
    "evaluate_model(grid_lgb, X_test, y_test)\n",
    "\n",
    "# 12. Print best parameters for each model\n",
    "print(\"Best Parameters for Random Forest:\", grid_rf.best_params_)\n",
    "print(\"Best Parameters for XGBoost:\", grid_xgb.best_params_)\n",
    "print(\"Best Parameters for LightGBM:\", grid_lgb.best_params_)"
   ],
   "id": "83bd3d6968e84b39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.5. Feature Importance",
   "id": "c7dfa751ec199698"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-03T16:27:13.890055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "importances = best_rf_model.named_steps['classifier'].feature_importances_\n",
    "plt.barh(range(len(importances)), importances)\n",
    "plt.show()"
   ],
   "id": "1cc672dea0e469b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Models\n",
    "1. XGBoost\n",
    "2. LightBGM\n",
    "3. Single decision tree - sklearn\n",
    "4. Bagging - sklearn\n",
    "5. Random Forest - sklearn\n",
    "\n",
    "Draw Tree in the end for the best performance model"
   ],
   "id": "7efa1a10b2f6591e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Deploy the machine learning model",
   "id": "7fd3ad94889830d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Improvement",
   "id": "49032d9e32da784d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Our data is imbalanced and can show certain sampling biases. This will \n",
    "increase the model's incorrect predictions. A more representative and \n",
    "careful data collection with proper definitions will help.\n",
    "2. More related data about symptoms/discomfort/previous medical history can \n",
    "help develop a model that is better adjusted for the use case.\n",
    "3. T-test and Chi-square test to thoroughly test differences between stroke/no \n",
    "stroke and bmi subpopulation.\n",
    "4. Feature engineering - split bimodal avg_glucose_level into 2 features\n",
    "5. To address imbalanced data try resampling techniques, like oversampling and \n",
    "under sampling; try adjusting class weights; try adjusting decision threshold\n",
    "6. "
   ],
   "id": "b7fe35df09a021f3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
